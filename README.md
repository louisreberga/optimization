# Optimization for Machine Learning
This repository is the final project of [IASD](https://www.lamsade.dauphine.fr/wp/iasd/) Optimization for Machine Learning course. The objective of the project was to implement from scratch and compare all the optimization methods and validate the theorems we saw during the course. 

Here is the list of the implemented methods: 
- Gradient Descent
- Gradient Descent using Armijo's step
- Nesterov Accelerated Gradient
- Polak-Ribi√®re Conjugate Gradient
- Newton (2nd order Gradient Descent)
- Stochastic Gradient
- Ridge Regularization
- Lasso Regularization (ISTA)
